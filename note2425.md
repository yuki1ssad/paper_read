# 模板



### ___

> 作者：

> 代码：

> 贡献：



> 方法：



> 总结：





# MLLM

## OneLLM: One Framework to Align All Modalities with Language_CVPR2024

> 作者：港中大, 上海AI Lab

> 代码：https://github.com/csuhan/OneLLM

> 贡献：

多模态大型语言模型（MLLM）由于其较强的多模态理解能力而得到了广泛的关注。

然而，现有的工作**严重依赖于特定于模态的编码器**，这些编码器通常在架构上有所不同，并且仅限于常见的模式。

之前工作的缺点：

1. 特定于模态的 encoder 通常在架构上有所不同，需要付出大量的努力将它们统一到一个单一的框架中；

2. 性能比较好的的预训练编码器通常仅限于广泛使用的模式，如图像、音频和视频。这一限制了 MLLM 扩展到更多模态的能力

<img src="https://raw.githubusercontent.com/yuki1ssad/paper_read/main/imgs/image-20240312120944265.png" alt="image-20240312120944265" style="zoom:80%;" />

在本文中，作者提出==OneLLM==，一个使用统一框架将==八种模态==连接到语言模态中的MLLM。通过一个**统一的多模态编码器**和一个**渐进的多模态对齐管道模块（ universal projection module (UPM)）**来实现这一点。

详细地说，首先训练一个图像投影模块来连接视觉编码器和 LLM。然后，通过混合多个图像投影模块和动态路由，建立了一个通用投影模块（UPM）。最后，逐步通过 UPM 将更多的模态对齐到 LLM 。

为了充分利用OneLLM在以下指令中的潜力，作者还设计了一个全面的多模态指令数据集，包括 2M 个来自图像、音频、视频、点云、深度/法线图、IMU 和 fMRI 大脑活动的数据。OneLLM 在25个不同的基准上进行了评估，包括多模态字幕生成、问题回答和推理等任务，表现出出色的性能。

> 创新点：

1. 提出了一个统一的框架来调整多模态输入与语言
2. OneLLM 是第一个在单个模型中集成了八个不同模态的 MLLM。
3. 提出了一个大规模的多模态指令数据集。在该数据集上微调的 OneLLM 在多模态任务上取得了优越的性能，优于专业模型和现有的MLLM

> 方法：

**OneLLM：**

1. **轻量级的模态 tokenizers** -> 2D/1D convolution layer，将输入信号转换为 tokens 序列，以便送入  transformer-based encoder 进行编码。

   ​	对于带有二维位置信息的视觉输入（图像和视频等），直接使用单一的二维卷积层作为 tokenizer。对于其他模态，将输入转换为	二维或一维序列，然后使用二维/一维卷积层进行序列化；

   ​	例如，将音频信号转换为二维谱图；以二维几何先验采样点云子集。

2. **一个通用的 encoder** ->  CLIP-ViT

3. **一个通用的投射模块 UPM**（多专家动态混合）

3. **一个 LLM**

![image-20240312121947963](https://raw.githubusercontent.com/yuki1ssad/paper_read/main/imgs/image-20240312121947963.png)

* 渐进式多模态对齐 progressive multimodal alignment
  * **训练参数：tokenizers、UPM**
  * 要对齐各个模态的数据：直白的方式是将所有模态的数据放在一块训练。然而这样会由于各种模态的数据规模不同，训练出来的模型会 bias to 数据量大的模态。
  * 因此，作者先训练一个 image-to-text 的模型作为 base model，然后在这基础上逐步地囊括其他模态到 LLM 中。其实就是借助图文对数据量大的优势训练一个好的 image-to-text 的投射器 $P_I$，并作为其他模态到语言的投射器的 initialization。
  * **Multimodal-Text Alignment.** 作者将多模态到文本的对齐作为一个持续的学习过程（增量学习），为了防止灾难性遗忘，将从以前的训练数据和当前数据中均匀抽样。
* 统一多模态指令调优 unified multimodal instruction tuning
  * **训练参数：LLM**
  * 在多模态-文本对齐之后，OneLLM 成为了一个多模态描述生成模型，它可以为任何输入生成一个简短的描述。为了充分释放 OneLLM 的多模态理解和推理能力，我们设计了一个大规模的多模态指令调优数据集，以进一步微调 OneLLM。

> 总结：

文中提到的局限性：

* 缺乏图像模态以外的大规模、高质量的数据集，这导致 OneLLM 和这些模态的专门模型之间存在一定的差距。
* 对高分辨率图像、长序列视频、音频等方面的细粒度多模态理解。在未来，作者将收集高质量的数据集，并设计新的编码器来实现细粒度的多模态理解，例如，支持不同长度的输入。

思考：

* 亮点：多模态向文本对齐使用增量学习的方式，这样以后再增加模态会比较方便。但是关于样本的选择（防遗忘）存在量的选择的问题。

* transformer 结构强大的表达能力，堆叠带来的性能增益的上限貌似还没有出现？
* 大模型比到后面，方法的改进带来的收益可能比较小，高质量数据将决定大模型的性能瓶颈